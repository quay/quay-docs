:_mod-docs-content-type: PROCEDURE
[id="restoring-up-red-hat-quay"]
= Restoring {productname}

Use the following procedures to restore {productname} when the {productname} Operator manages the database. It should be performed after a backup of your {productname} registry has been performed. See xref:backing-up-red-hat-quay-operator.adoc#backing-up-red-hat-quay-operator[Backing up {productname}] for more information.

.Prerequisites

* {productname} is deployed on {ocp} using the {productname} Operator.
* A backup of the {productname} configuration managed by the {productname} Operator has been created following the instructions in the xref:backing-up-red-hat-quay-operator.adoc#backing-up-red-hat-quay-operator[Backing up {productname}] section
* Your {productname} database has been backed up.
* The object storage bucket used by {productname} has been backed up.
* The components `quay`, `postgres` and `objectstorage` are set to `managed: true`
* If the component `clair` is set to `managed: true`, the component `clairpostgres` is also set to `managed: true` (starting with {productname} v3.7 or later)
* There is no running {productname} deployment managed by the {productname} Operator in the target namespace on your {ocp} cluster

[NOTE]
====
If your deployment contains partially unmanaged database or storage components and you are using external services for PostgreSQL or S3-compatible object storage to run your {productname} deployment, you must refer to the service provider or vendor documentation to restore their data from a backup prior to restore {productname}
====

[id="restoring-quay-and-configuration-from-backup"]
== Restoring {productname} and its configuration from a backup

Use the following procedure to restore {productname} and its configuration files from a backup.

[NOTE]
====
These instructions assume you have followed the process in the xref:backing-up-red-hat-quay-operator.adoc#backing-up-red-hat-quay-operator[Backing up {productname}] guide and create the backup files with the same names.
====

.Procedure

. Restore the backed up {productname} configuration by entering the following command:
+
[source,terminal]
----
$ oc create -f ./config-bundle.yaml
----
+
[IMPORTANT]
====
If you receive the error `Error from server (AlreadyExists): error when creating "./config-bundle.yaml": secrets "config-bundle-secret" already exists`, you must delete your existing resource with `$ oc delete Secret config-bundle-secret -n <quay-namespace>` and recreate it with `$ oc create -f ./config-bundle.yaml`.
====

. Restore the generated keys from the backup by entering the following command:
+
[source,terminal]
----
$ oc create -f ./managed-secret-keys.yaml
----

. Restore the `QuayRegistry` custom resource:
+
[source,terminal]
----
$ oc create -f ./quay-registry.yaml
----

. Check the status of the {productname} deployment and wait for it to be available:
+
[source,terminal]
----
$ oc wait quayregistry registry --for=condition=Available=true -n <quay-namespace>
----

[id="scale-down-quay-deployment"]
== Scaling down your {productname} deployment

Use the following procedure to scale down your {productname} deployment. 

.Procedure 

. Depending on the version of your {productname} deployment, scale down your deployment using one of the following options. 

.. *For Operator version 3.7 and newer:* Scale down the {productname} deployment by disabling auto scaling and overriding the replica count for Quay, mirror workers and Clair (if managed). Your `QuayRegistry` resource should look similar to the following:
+
[source,yaml]
----
apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: registry
  namespace: ns
spec:
  components:
    …
    - kind: horizontalpodautoscaler
      managed: false <1>
    - kind: quay
      managed: true
      overrides: <2>
        replicas: 0
    - kind: clair
      managed: true
      overrides:
        replicas: 0
    - kind: mirror
      managed: true
      overrides:
        replicas: 0
    …
----
<1> Disable auto scaling of Quay, Clair and Mirroring workers
<2> Set the replica count to 0 for components accessing the database and objectstorage

.. *For Operator version 3.6 and earlier:* Scale down the {productname} deployment by scaling down the {productname} registry first and then the managed {productname} resources:
+
[source,terminal]
----
$ oc scale --replicas=0 deployment $(oc get deployment -n <quay-operator-namespace>|awk '/^quay-operator/ {print $1}') -n <quay-operator-namespace>
----
+
[source,terminal]
----
$ oc scale --replicas=0 deployment $(oc get deployment -n <quay-namespace>|awk '/quay-app/ {print $1}') -n <quay-namespace>
----
+
[source,terminal]
----
$ oc scale --replicas=0 deployment $(oc get deployment -n <quay-namespace>|awk '/quay-mirror/ {print $1}') -n <quay-namespace>
----
+
[source,terminal]
----
$ oc scale --replicas=0 deployment $(oc get deployment -n <quay-namespace>|awk '/clair-app/ {print $1}') -n <quay-namespace>
----

. Wait for the `registry-quay-app`, `registry-quay-mirror` and `registry-clair-app` pods (depending on which components you set to be managed by {productname} Operator) to disappear. You can check their status by running the following command:
+
[source,terminal]
----
$ oc get pods -n <quay-namespace>
----
+
Example output:
+
[source,terminal]
----
registry-quay-config-editor-77847fc4f5-nsbbv   1/1     Running            0          9m1s
registry-quay-database-66969cd859-n2ssm        1/1     Running            0          6d1h
registry-quay-redis-7cc5f6c977-956g8           1/1     Running            0          5d21h
----

[id="restoring-quay-database"]
== Restoring your {productname} database

Use the following procedure to restore your {productname} database. 

.Procedure 

. Identify your `Quay` database pod by entering the following command:
+
[source,terminal]
----
$ oc get pod -l quay-component=postgres -n  <quay-namespace> -o jsonpath='{.items[0].metadata.name}'
----
+
Example output:
+
----
quayregistry-quay-database-59f54bb7-58xs7
----

. Upload the backup by copying it from the local environment and into the pod:
+
----
$ oc cp ./backup.sql -n <quay-namespace> registry-quay-database-66969cd859-n2ssm:/tmp/backup.sql
----

. Open a remote terminal to the database by entering the following command:
+
[source,terminal]
----
$ oc rsh -n <quay-namespace> registry-quay-database-66969cd859-n2ssm
----

. Enter psql by running the following command:
+
[source,terminal]
----
bash-4.4$ psql
----

. You can list the database by running the following command:
+
----
postgres=# \l
----
+
.Example output
+
[source,terminal]
----
                                                  List of databases
           Name            |           Owner            | Encoding |  Collate   |   Ctype    |   Access privileges
----------------------------+----------------------------+----------+------------+------------+-----------------------
postgres                   | postgres                   | UTF8     | en_US.utf8 | en_US.utf8 |
quayregistry-quay-database | quayregistry-quay-database | UTF8     | en_US.utf8 | en_US.utf8 |
----

. Drop the database by entering the following command:
+
[source,terminal]
----
postgres=# DROP DATABASE "quayregistry-quay-database";
----
+
.Example output
+
[source,terminal]
----
DROP DATABASE
----

. Exit the postgres CLI to re-enter bash-4.4:
+
[source,terminal]
----
\q
----

. Redirect your PostgreSQL database to your backup database:
+
[source,terminal]
----
sh-4.4$ psql < /tmp/backup.sql
----

. Exit bash by entering the following command:
+
[source,terminal]
----
sh-4.4$ exit
----

[id="restoring-quay-object-storage-data"]
== Restore your {productname} object storage data

Use the following procedure to restore your {productname} object storage data. 

.Procedure

. Export the `AWS_ACCESS_KEY_ID` by entering the following command:
+
[source,terminal]
----
$ export AWS_ACCESS_KEY_ID=$(oc get secret -l app=noobaa -n <quay-namespace>  -o jsonpath='{.items[0].data.AWS_ACCESS_KEY_ID}' |base64 -d)
----

. Export the `AWS_SECRET_ACCESS_KEY` by entering the following command:
+
[source,terminal]
----
$ export AWS_SECRET_ACCESS_KEY=$(oc get secret -l app=noobaa -n <quay-namespace> -o jsonpath='{.items[0].data.AWS_SECRET_ACCESS_KEY}' |base64 -d)
----

. Upload all blobs to the bucket by running the following command:
+
[source,terminal]
----
$ aws s3 sync --no-verify-ssl --endpoint https://$(oc get route s3 -n openshift-storage  -o jsonpath='{.spec.host}') ./blobs  s3://$(oc get cm -l app=noobaa -n <quay-namespace> -o jsonpath='{.items[0].data.BUCKET_NAME}')
----

[NOTE]
====
You can also use link:https://rclone.org/[rclone] or link:https://s3tools.org/s3cmd[sc3md] instead of the AWS command line utility.
====

[id="scaling-up-quay"]
== Scaling up your {productname} deployment

. Depending on the version of your {productname} deployment, scale up your deployment using one of the following options. 

.. *For Operator version 3.7 and newer:* Scale up the {productname} deployment by re-enabling auto scaling, if desired, and removing the replica overrides for Quay, mirror workers and Clair as applicable. Your `QuayRegistry` resource should look similar to the following:
+
[source,yaml]
----
apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  name: registry
  namespace: ns
spec:
  components:
    …
    - kind: horizontalpodautoscaler
      managed: true <1>
    - kind: quay <2>
      managed: true
    - kind: clair
      managed: true
    - kind: mirror
      managed: true
    …
----
<1> Re-enables auto scaling of {productname}, Clair and mirroring workers again (if desired)
<2> Replica overrides are removed again to scale the {productname} components back up

.. *For Operator version 3.6 and earlier:* Scale up the {productname} deployment by scaling up the {productname} registry again:
+
[source,terminal]
----
$ oc scale --replicas=1 deployment $(oc get deployment -n <quay-operator-namespace> | awk '/^quay-operator/ {print $1}') -n <quay-operator-namespace>
----

. Check the status of the {productname} deployment:
+
[source,terminal]
----
$ oc wait quayregistry registry --for=condition=Available=true -n <quay-namespace>
----
+
Example output:
+
[source,yaml]
----
apiVersion: quay.redhat.com/v1
kind: QuayRegistry
metadata:
  ...
  name: registry
  namespace: <quay-namespace>
  ...
spec:
  ...
status:
  - lastTransitionTime: '2022-06-20T05:31:17Z'
    lastUpdateTime: '2022-06-20T17:31:13Z'
    message: All components reporting as healthy
    reason: HealthChecksPassing
    status: 'True'
    type: Available
----
:_content-type: PROCEDURE
[id="operator-standalone-object-gateway"]
= Leveraging the Multicloud Object Gateway Component in the {odf} Operator for {productname}

As part of a {productname} subscription, users are entitled to use the _Multicloud Object Gateway_ component of the {odf} Operator (formerly known as OpenShift Container Storage Operator). This gateway component allows you to provide an S3-compatible object storage interface to {productname} backed by Kubernetes `PersistentVolume`-based block storage. The usage is limited to a {productname} deployment managed by the Operator and to the exact specifications of the multicloud Object Gateway instance as documented below.

Since {productname} does not support local filesystem storage, users can leverage the gateway in combination with Kubernetes `PersistentVolume` storage instead, to provide a supported deployment. A `PersistentVolume` is directly mounted on the gateway instance as a backing store for object storage and any block-based `StorageClass` is supported.

By the nature of `PersistentVolume`, this is not a scale-out, highly available solution and does not replace a scale-out storage system like {odf}. Only a single instance of the gateway is running. If the pod running the gateway becomes unavailable due to rescheduling, updates or unplanned downtime, this will cause temporary degradation of the connected {productname} instances.

Using the following procedures, you will install the Local Storage Operator, {odf}, and create a standalone Multicloud Object Gateway to deploy {productname} on {ocp}. 

[NOTE]
====
The following documentation shares commonality with the official link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_bare_metal_infrastructure/deploy-standalone-multicloud-object-gateway#doc-wrapper[{odf} documentation].
====


[id="installing-local-storage-operator"]
== Installing the Local Storage Operator on {ocp}

Use the following procedure to install the Local Storage Operator from the *Operator Hub* before creating {odf} clusters on local storage devices. 

. Log in to the *OpenShift Web Console*.

. Click *Operators* → *OperatorHub*.

. Type *local storage* into the search box to find the Local Storage Operator from the list of Operators. Click *Local Storage*. 

. Click *Install*.

. Set the following options on the Install Operator page:
+
* For Update channel, select *stable*. 
* For Installation mode, select *A specific namespace on the cluster*. 
* For Installed Namespace, select *Operator recommended namespace openshift-local-storage*. 
* For Update approval, select *Automatic*. 

. Click *Install*. 

[id="installing-odf"]
== Installing {odf} on {ocp}

Use the following procedure to install {odf} on {ocp}. 

.Prerequisites 

* Access to an {ocp} cluster using an account with `cluster-admin` and Operator installation permissions.
* You must have at least three worker nodes in the {ocp} cluster.
* For additional resource requirements, see the link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html-single/planning_your_deployment/index[Planning your deployment] guide. 

.Procedure 

. Log in to the *OpenShift Web Console*.

. Click *Operators* → *OperatorHub*.

. Type *OpenShift Data Foundation* in the search box. Click *OpenShift Data Foundation*. 

. Click *Install*. 

. Set the following options on the Install Operator page:
+
* For Update channel, select the most recent stable version. 
* For Installation mode, select *A specific namespace on the cluster*. 
* For Installed Namespace, select *Operator recommended Namespace: openshift-storage*. 
* For Update approval, select *Automatic* or *Manual*. 
+
If you select *Automatic* updates, then the Operator Lifecycle Manager (OLM) automatically upgrades the running instance of your Operator without any intervention.
+
If you select *Manual* updates, then the OLM creates an update request. As a cluster administrator, you must then manually approve that update request to update the Operator to a newer version.

* For Console plugin, select *Enable*. 

. Click *Install*. 
+
After the Operator is installed, a pop-up with a message, `Web console update is available` appears on the user interface. Click *Refresh web console* from this pop-up for the console changes to reflect. 

. Continue to the following section, "Creating a standalone Multicloud Object Gateway", to leverage the Multicloud Object Gateway Component for {productname}.

[id="creating-mcg"]
== Creating a standalone Multicloud Object Gateway using the {ocp} UI

Use the following procedure to create a standalone Multicloud Object Gateway. 

.Prerequisites 

* You have installed the Local Storage Operator.
* You have installed the {odf} Operator. 

.Procedure 

. In the *OpenShift Web Console*, click *Operators* -> *Installed Operators* to view all installed Operators.
+
Ensure that the namespace is `openshift-storage`. 

. Click *Create StorageSystem*. 

. On the *Backing storage* page, select the following:
.. Select *Multicloud Object Gateway* for *Deployment type*. 
.. Select the *Create a new StorageClass using the local storage devices* option. 
.. Click *Next*. 
+
[NOTE]
====
You are prompted to install the Local Storage Operator if it is not already installed. Click *Install*, and follow the procedure as described in "Installing the Local Storage Operator on {ocp}". 
====

. On the *Create local volume set* page, provide the following information:
.. Enter a name for the *LocalVolumeSet* and the *StorageClass*. By default, the local volume set name appears for the storage class name. You can change the name.
.. Choose one of the following:
+
* *Disk on all nodes*
+
Uses the available disks that match the selected filters on all the nodes. 
+
* *Disk on selected nodes* 
+
Uses the available disks that match the selected filters only on the selected nodes.

.. From the available list of *Disk Type*, select *SSD/NVMe*. 

.. Expand the *Advanced* section and set the following options:
+
|===
|*Volume Mode* | Filesystem is selected by default. Always ensure that Filesystem is selected for Volume Mode. 
|*Device Type* | Select one or more device type from the dropdown list.
|*Disk Size*| Set a minimum size of 100GB for the device and maximum available size of the device that needs to be included.
|*Maximum Disks Limit* | This indicates the maximum number of PVs that can be created on a node. If this field is left empty, then PVs are created for all the available disks on the matching nodes.
|===

.. Click *Next*
+
A pop-up to confirm the creation of `LocalVolumeSet` is displayed. 

.. Click *Yes* to continue. 

. In the *Capacity and nodes* page, configure the following:
+
.. *Available raw capacity* is populated with the capacity value based on all the attached disks associated with the storage class. This takes some time to show up. The *Selected nodes* list shows the nodes based on the storage class.
.. Click *Next* to continue. 

. Optional. Select the *Connect to an external key management service* checkbox. This is optional for cluster-wide encryption. 
.. From the *Key Management Service Provider* drop-down list, either select *Vault* or *Thales CipherTrust Manager (using KMIP)*. If you selected *Vault*, go to the next step. If you selected *Thales CipherTrust Manager (using KMIP)*, go to step iii.
.. Select an *Authentication Method*. 
+
Using Token Authentication method
+
* Enter a unique *Connection Name*, host *Address* of the Vault server ('https://<hostname or ip>'), *Port* number and *Token*.
+
* Expand *Advanced Settings* to enter additional settings and certificate details based on your `Vault` configuration:
+
** Enter the Key Value secret path in *Backend Path* that is dedicated and unique to OpenShift Data Foundation.
** Optional: Enter *TLS Server Name* and *Vault Enterprise Namespace*.
** Upload the respective PEM encoded certificate file to provide the *CA Certificate*, *Client Certificate,* and *Client Private Key*.
** Click *Save* and skip to step iv.
+
Using Kubernetes authentication method 
+
* Enter a unique Vault *Connection Name*, host *Address* of the Vault server ('https://<hostname or ip>'), *Port* number and *Role* name.
* Expand *Advanced Settings* to enter additional settings and certificate details based on your Vault configuration:
** Enter the Key Value secret path in *Backend Path* that is dedicated and unique to {odf}.
** Optional: Enter *TLS Server Name* and *Authentication Path* if applicable.
** Upload the respective PEM encoded certificate file to provide the *CA Certificate*, *Client Certificate*, and *Client Private Key*.
** Click *Save* and skip to step iv.

.. To use *Thales CipherTrust Manager (using KMIP)* as the KMS provider, follow the steps below:

... Enter a unique *Connection Name* for the Key Management service within the project.
... In the *Address* and *Port* sections, enter the IP of Thales CipherTrust Manager and the port where the KMIP interface is enabled. For example:
+
* *Address*: 123.34.3.2
* *Port*: 5696
... Upload the *Client Certificate*, *CA certificate*, and *Client Private Key*.
... If StorageClass encryption is enabled, enter the Unique Identifier to be used for encryption and decryption generated above.
... The *TLS Server* field is optional and used when there is no DNS entry for the KMIP endpoint. For example,`kmip_all_<port>.ciphertrustmanager.local`.

.. Select a *Network*. 
.. Click *Next*. 

. In the *Review and create* page, review the configuration details. To modify any configuration settings, click *Back*. 

. Click *Create StorageSystem*. 


[id="creating-standalone-object-gateway"]
== Create A standalone Multicloud Object Gateway using the CLI

Use the following procedure to install the {odf} (formerly known as OpenShift Container Storage) Operator and configure a single instance Multi-Cloud Gateway service.

[NOTE]
====
The following configuration cannot be run in parallel on a cluster with {odf} installed.
====

.Procedure

. On the *OpenShift Web Console*, and then select *Operators* -> *OperatorHub*. 

. Search for *{odf}*, and then select *Install*. 

. Accept all default options, and then select *Install*. 

. Confirm that the Operator has installed by viewing the *Status* column, which should be marked as *Succeeded*. 
+
[WARNING]
====
When the installation of the {odf} Operator is finished, you are prompted to create a storage system. Do not follow this instruction. Instead, create NooBaa object storage as outlined the following steps.
====

. On your machine, create a file named `noobaa.yaml` with the following information:
+
[source,yaml]
+
----
apiVersion: noobaa.io/v1alpha1
kind: NooBaa
metadata:
  name: noobaa
  namespace: openshift-storage
spec:
 dbResources:
   requests:
     cpu: '0.1'
     memory: 1Gi
 dbType: postgres
 coreResources:
   requests:
     cpu: '0.1'
     memory: 1Gi
----
+
This creates a single instance deployment of the _Multi-cloud Object Gateway_.

. Apply the configuration with the following command:
+
[source,terminal]
----
$ oc create -n openshift-storage -f noobaa.yaml
----
+
.Example output
+
[source,terminal]
----
noobaa.noobaa.io/noobaa created
----

. After a few minutes, the _Multi-cloud Object Gateway_ should finish provisioning. You can enter the following command to check its status:
+
[source,terminal]
----
$ oc get -n openshift-storage noobaas noobaa -w
----
+
.Example output
+
[source,terminal]
----
NAME     MGMT-ENDPOINTS              S3-ENDPOINTS                IMAGE                                                                                                            PHASE   AGE
noobaa   [https://10.0.32.3:30318]   [https://10.0.32.3:31958]   registry.redhat.io/ocs4/mcg-core-rhel8@sha256:56624aa7dd4ca178c1887343c7445a9425a841600b1309f6deace37ce6b8678d   Ready   3d18h
----

. Configure a backing store for the gateway by creating the following YAML file, named  `noobaa-pv-backing-store.yaml`:
+
[source,yaml]
----
apiVersion: noobaa.io/v1alpha1
kind: BackingStore
metadata:
  finalizers:
  - noobaa.io/finalizer
  labels:
    app: noobaa
  name: noobaa-pv-backing-store
  namespace: openshift-storage
spec:
  pvPool:
    numVolumes: 1
    resources:
      requests:
        storage: 50Gi <1>
    storageClass: STORAGE-CLASS-NAME <2>
  type: pv-pool
----
<1> The overall capacity of the object storage service. Adjust as needed.
<2> The `StorageClass` to use for the `PersistentVolumes` requested. Delete this property to use the cluster default.

. Enter the following command to apply the configuration:
+
[source,terminal]
----
$ oc create -f noobaa-pv-backing-store.yaml
----
+
.Example output
+
[source,terminal]
----
backingstore.noobaa.io/noobaa-pv-backing-store created
----
+
This creates the backing store configuration for the gateway. All images in {productname} will be stored as objects through the gateway in a `PersistentVolume` created by the above configuration.

. Run the following command to make the `PersistentVolume` backing store the default for all `ObjectBucketClaims` issued by the {productname} Operator:
+
[source,terminal]
----
$ oc patch bucketclass noobaa-default-bucket-class --patch '{"spec":{"placementPolicy":{"tiers":[{"backingStores":["noobaa-pv-backing-store"]}]}}}' --type merge -n openshift-storage
----

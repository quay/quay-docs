[id="operator-upgrade"]
= Upgrading the {productname} Operator Overview

[NOTE]
====
Currently, upgrading the {productname} Operator is not supported on IBM Power and IBM Z.
====

The {productname} Operator follows a _synchronized versioning_ scheme, which means that each version of the Operator is tied to the version of {productname} and the components that it manages. There is no field on the `QuayRegistry` custom resource which sets the version of {productname} to `deploy`; the Operator can only deploy a single version of all components. This scheme was chosen to ensure that all components work well together and to reduce the complexity of the Operator needing to know how to manage the lifecycles of many different versions of {productname} on Kubernetes.

[id="operator-lifecycle-manager"]
== Operator Lifecycle Manager

The {productname} Operator should be installed and upgraded using the link:https://docs.openshift.com/container-platform/{ocp-y}/operators/understanding/olm/olm-understanding-olm.html[Operator Lifecycle Manager (OLM)]. When creating a `Subscription` with the default `approvalStrategy: Automatic`, OLM will automatically upgrade the {productname} Operator whenever a new version becomes available.

[WARNING]
====
When the {productname} Operator is installed by Operator Lifecycle Manager, it might be configured to support automatic or manual upgrades. This option is shown on the *OperatorHub* page for the {productname} Operator during installation. It can also be found in the {productname} Operator `Subscription` object by the `approvalStrategy` field.  Choosing `Automatic` means that your {productname} Operator will automatically be upgraded whenever a new Operator version is released. If this is not desirable, then the `Manual` approval strategy should be selected.
====

[id="upgrading-quay-operator"]
== Upgrading the {productname} Operator

The standard approach for upgrading installed Operators on {ocp} is documented at link:https://docs.openshift.com/container-platform/{ocp-y}/operators/admin/olm-upgrading-operators.html[Upgrading installed Operators].

In general, {productname} supports upgrades from a prior (N-1) minor version only.  For example, upgrading directly from {productname} 3.0.5 to the latest version of 3.5 is not supported. Instead, users would have to upgrade as follows:

. 3.0.5 -> 3.1.3
. 3.1.3 -> 3.2.2
. 3.2.2 -> 3.3.4
. 3.3.4 -> 3.4.z
. 3.4.z -> 3.5.z

This is required to ensure that any necessary database migrations are done correctly and in the right order during the upgrade.

In some cases, {productname} supports direct, single-step upgrades from prior (N-2, N-3) minor versions. This simplifies the upgrade procedure for customers on older releases.  The following upgrade paths are supported for {productname} {producty}:

* 3.9.z -> 3.11.z
* 3.10.z -> 3.11.z

[NOTE]
====
Upgrading from 3.8.z to 3.11 is unsupported. Users must first upgrade to 3.9 or 3.10, and then upgrade to 3.11.
====

For users on standalone deployments of {productname} wanting to upgrade to 3.11, see the link:https://access.redhat.com/documentation/en-us/red_hat_quay/{producty}/html-single/upgrade_red_hat_quay/index#standalone_upgrade[Standalone upgrade] guide.

[id="upgrading-red-hat-quay"]
=== Upgrading {productname}

To update {productname} from one minor version to the next, for example, 3.10 -> 3.11, you must change the update channel for the {productname} Operator.

For `z` stream upgrades, for example, 3.10.1 -> 3.10.2, updates are released in the major-minor channel that the user initially selected during install. The procedure to perform a `z` stream upgrade depends on the `approvalStrategy` as outlined above. If the approval strategy is set to `Automatic`, the {productname} Operator upgrades automatically to the newest `z` stream. This results in automatic, rolling {productname} updates to newer `z` streams with little to no downtime. Otherwise, the update must be manually approved before installation can begin.

[id="config-editor-removal"]
== Removing config editor objects on {productname} Operator

The config editor has been removed from the {productname} Operator on {ocp} deployments. As a result, the `quay-config-editor` pod no longer deploys, and users cannot check the status of the config editor route. Additionally, the Config Editor Endpoint no longer generates on the {productname} Operator *Details* page. 

Users with existing {productname} Operators who are upgrading from 3.7, 3.8, or 3.9 to {producty} must manually remove the {productname} config editor by removing the `pod`, `deployment`, `route,` `service`, and `secret` objects.

To remove the `deployment`, `route,` `service`, and `secret` objects, use the following procedure. 

.Prerequisites 

* You have deployed {productname} version 3.7, 3.8, or 3.9.
* You have a valid `QuayRegistry` object.

.Procedure

. Obtain the `quayregistry-quay-config-editor` route object by entering the following command:
+
[source,terminal]
----
$ oc get route
----
+
.Example output
+
[source,terminal]
----
---
quayregistry-quay-config-editor-c866f64c4-68gtb   1/1     Running     0          49m
---
----

. Remove the `quayregistry-quay-config-editor` route object by entering the following command:
+
[source,terminal]
----
$ oc delete route quayregistry-quay-config-editor
----

. Obtain the `quayregistry-quay-config-editor` deployment object by entering the following command:
+
[source,terminal]
----
$ oc get deployment
----
+
.Example output
+
[source,terminal]
----
---
quayregistry-quay-config-editor
---
----

. Remove the `quayregistry-quay-config-editor` deployment object by entering the following command:
+
[source,terminal]
----
$ oc delete deployment quayregistry-quay-config-editor
----

. Obtain the `quayregistry-quay-config-editor` service object by entering the following command:
+
[source,terminal]
----
$ oc get svc | grep config-editor
----
+
.Example output
+
[source,terminal]
----
quayregistry-quay-config-editor   ClusterIP   172.30.219.194   <none>        80/TCP                              6h15m 
----

. Remove the `quayregistry-quay-config-editor` service object by entering the following command:
+
[source,terminal]
----
$ oc delete service quayregistry-quay-config-editor
----

. Obtain the `quayregistry-quay-config-editor-credentials` secret by entering the following command:
+
[source,terminal]
----
$ oc get secret | grep config-editor
----
+
.Example output
+
[source,terminal]
----
quayregistry-quay-config-editor-credentials-mb8kchfg92   Opaque                2       52m
----

. Delete the `quayregistry-quay-config-editor-credentials` secret by entering the following command:
+
[source,terminal]
----
$ oc delete secret quayregistry-quay-config-editor-credentials-mb8kchfg92
----

. Obtain the `quayregistry-quay-config-editor` pod by entering the following command:
+
[source,terminal]
----
$ $ oc get pod
----
+
.Example output
+
[source,terminal]
----
---
quayregistry-quay-config-editor-c866f64c4-68gtb   1/1     Running     0          49m
---
----

. Delete the `quayregistry-quay-config-editor` pod by entering the following command:
+
[source,terminal]
----
$ oc delete pod quayregistry-quay-app-6bc4fbd456-8bc9c
----

////
[id="upgrading-postgresql-databases"]
=== Updating {productname} from 3.8 -> 3.9

[IMPORTANT]
====
If your {productname} deployment is upgrading from one y-stream to the next, for example, from 3.8.10 -> 3.8.11, you must not switch the upgrade channel from `stable-3.8` to `stable-3.9`. Changing the upgrade channel in the middle of a y-stream upgrade will disallow {productname} from upgrading to 3.9. This is a known issue and will be fixed in a future version of {productname}. 
====

When updating {productname} 3.8 -> 3.9, the Operator automatically upgrades the existing PostgreSQL databases for Clair and {productname} from version 10 to version 13. 

[IMPORTANT]
====
* This upgrade is irreversible. It is highly recommended that you upgrade to PostgreSQL 13. PostgreSQL 10 had its final release on November 10, 2022 and is no longer supported. For more information, see the link:https://www.postgresql.org/support/versioning/[PostgreSQL Versioning Policy]. 
* By default, {productname} is configured to remove old persistent volume claims (PVCs) from PostgreSQL 10. To disable this setting and backup old PVCs, you must set `POSTGRES_UPGRADE_RETAIN_BACKUP` to `True` in your `quay-operator` `Subscription` object. 
====

.Prerequisites 

* You have installed {productname} 3.8 on {ocp}. 
* 100 GB of free, additional storage.
+
During the upgrade process, additional persistent volume claims (PVCs) are provisioned to store the migrated data. This helps prevent a destructive operation on user data. The upgrade process rolls out PVCs for 50 GB for both the {productname} database upgrade, and the Clair database upgrade. 

.Procedure

. Optional. Back up your old PVCs from PostgreSQL 10 by setting `POSTGRES_UPGRADE_RETAIN_BACKUP` to `True` your `quay-operator` `Subscription` object. For example:
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: quay-operator
  namespace: quay-enterprise
spec:
  channel: stable-3.8
  name: quay-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  config:
    env: 
    - name: POSTGRES_UPGRADE_RETAIN_BACKUP
      value: "true"
----

. In the {ocp} Web Console, navigate to *Operators* -> *Installed Operators*. 

. Click on the {productname} Operator. 

. Navigate to the *Subscription* tab. 

. Under *Subscription details* click *Update channel*. 

. Select *stable-3.9* and save the changes. 

. Check the progress of the new installation under *Upgrade status*. Wait until the upgrade status changes to *1 installed* before proceeding. 

. In your {ocp} cluster, navigate to *Workloads* -> *Pods*. Existing pods should be terminated, or in the process of being terminated. 

. Wait for the following pods, which are responsible for upgrading the database and alembic migration of existing data, to spin up: `clair-postgres-upgrade`, `quay-postgres-upgrade`, and `quay-app-upgrade`. 

. After the `clair-postgres-upgrade`, `quay-postgres-upgrade`, and `quay-app-upgrade` pods are marked as *Completed*, the remaining pods for your {productname} deployment spin up. This takes approximately ten minutes. 

. Verify that the `quay-database` and `clair-postgres` pods now use the `postgresql-13` image. 

. After the `quay-app` pod is marked as *Running*, you can reach your {productname} registry. 


[id="upgrade-33-36"]
=== Upgrading directly from 3.3.z or 3.4.z to 3.6

The following section provides important information when upgrading from {productname} 3.3.z or 3.4.z to 3.6. 

[id="upgrading-edge-routing-enabled"]
==== Upgrading with edge routing enabled

* Previously, when running a 3.3.z version of {productname} with edge routing enabled, users were unable to upgrade to 3.4.z versions of {productname}. This has been resolved with the release of {productname} 3.6.

* When upgrading from 3.3.z to 3.6, if `tls.termination` is set to `none` in your {productname} 3.3.z deployment, it will change to HTTPS with TLS edge termination and use the default cluster wildcard certificate. For example:
+
[source,yaml]
----
apiVersion: redhatcop.redhat.io/v1alpha1
kind: QuayEcosystem
metadata:
  name: quay33
spec:
  quay:
    imagePullSecretName: redhat-pull-secret
    enableRepoMirroring: true
    image: quay.io/quay/quay:v3.3.4-2
    ...
    externalAccess:
      hostname: quayv33.apps.devcluster.openshift.com
      tls:
        termination: none
    database:
...
----
////

[id="upgrading-with-tls-cert-key-pairs-without-san"]
==== Upgrading with custom SSL/TLS certificate/key pairs without Subject Alternative Names

There is an issue for customers using their own SSL/TLS certificate/key pairs without Subject Alternative Names (SANs) when upgrading from {productname} 3.3.4 to {productname} 3.6 directly. During the upgrade to {productname} 3.6, the deployment is blocked, with the error message from the {productname} Operator pod logs indicating that the {productname} SSL/TLS certificate must have SANs.

If possible, you should regenerate your SSL/TLS certificates with the correct hostname in the SANs. A possible workaround involves defining an environment variable in the `quay-app`, `quay-upgrade` and `quay-config-editor` pods after upgrade to enable CommonName matching:

----
 GODEBUG=x509ignoreCN=0
----

The `GODEBUG=x509ignoreCN=0` flag enables the legacy behavior of treating the CommonName field on X.509 certificates as a hostname when no SANs are present. However, this workaround is not recommended, as it will not persist across a redeployment.

////

[id="configuring-clair-v4-upgrading-from-33-34-to-36"]
==== Configuring Clair v4 when upgrading from 3.3.z or 3.4.z to 3.6 using the {productname} Operator

To set up Clair v4 on a new {productname} deployment on {ocp}, it is highly recommended to use the {productname} Operator. By default, the {productname} Operator will install or upgrade a Clair deployment along with your {productname} deployment and configure Clair automatically.

//link needs replaced
For instructions about setting up Clair v4 in a disconnected {ocp} cluster, see link:https://access.redhat.com/documentation/en-us/red_hat_quay/{producty}/html-single/manage_red_hat_quay/index#clair-openshift[Setting Up Clair on a {productname} OpenShift deployment].

[id="swift-config-upgrading-from-33-to-36"]
=== Swift configuration when upgrading from 3.3.z to 3.6

When upgrading from {productname} 3.3.z to 3.6.z, some users might receive the following error: `Switch auth v3 requires tenant_id (string) in os_options`. As a workaround, you can manually update your `DISTRIBUTED_STORAGE_CONFIG` to add the `os_options` and `tenant_id` parameters:

[source,yaml]
----
  DISTRIBUTED_STORAGE_CONFIG:
    brscale:
    - SwiftStorage
    - auth_url: http://****/v3
      auth_version: "3"
      os_options:
        tenant_id: ****
        project_name: ocp-base
        user_domain_name: Default
      storage_path: /datastorage/registry
      swift_container: ocp-svc-quay-ha
      swift_password: *****
      swift_user: *****
----
////

[id="changing-update-channel-for-operator"]
=== Changing the update channel for the {productname} Operator

The subscription of an installed Operator specifies an update channel, which is used to track and receive updates for the Operator. To upgrade the {productname} Operator to start tracking and receiving updates from a newer channel, change the update channel in the *Subscription* tab for the installed {productname} Operator. For subscriptions with an `Automatic` approval strategy, the upgrade begins automatically and can be monitored on the page that lists the Installed Operators.

[id="manually-approving-pending-operator-upgrade"]
=== Manually approving a pending Operator upgrade

If an installed Operator has the approval strategy in its subscription set to `Manual`, when new updates are released in its current update channel, the update must be manually approved before installation can begin. If the {productname} Operator has a pending upgrade, this status will be displayed in the list of Installed Operators. In the `Subscription` tab for the {productname} Operator, you can preview the install plan and review the resources that are listed as available for upgrade. If satisfied, click `Approve` and return to the page that lists Installed Operators to monitor the progress of the upgrade.

The following image shows the *Subscription* tab in the UI, including the update `Channel`, the `Approval` strategy, the `Upgrade status` and the `InstallPlan`:

image:update-channel-approval-strategy.png[Subscription tab including upgrade Channel and Approval strategy]

The list of Installed Operators provides a high-level summary of the current Quay installation:

image:installed-operators-list.png[Installed Operators]

[id="upgrading-quayregistry"]
== Upgrading a QuayRegistry resource

When the {productname} Operator starts, it immediately looks for any `QuayRegistries` it can find in the namespace(s) it is configured to watch. When it finds one, the following logic is used:

* If `status.currentVersion` is unset, reconcile as normal.
* If `status.currentVersion` equals the Operator version, reconcile as normal.
* If `status.currentVersion` does not equal the Operator version, check if it can be upgraded. If it can, perform upgrade tasks and set the `status.currentVersion` to the Operator's version once complete. If it cannot be upgraded, return an error and leave the `QuayRegistry` and its deployed Kubernetes objects alone.

[id="upgrading-quayecosystem"]
== Upgrading a QuayEcosystem

Upgrades are supported from previous versions of the Operator which used the `QuayEcosystem` API for a limited set of configurations. To ensure that migrations do not happen unexpectedly, a special label needs to be applied to the `QuayEcosystem` for it to be migrated. A new `QuayRegistry` will be created for the Operator to manage, but the old `QuayEcosystem` will remain until manually deleted to ensure that you can roll back and still access Quay in case anything goes wrong. To migrate an existing `QuayEcosystem` to a new `QuayRegistry`, use the following procedure. 

.Procedure

. Add `"quay-operator/migrate": "true"` to the `metadata.labels` of the `QuayEcosystem`.
+
[source,terminal]
----
$ oc edit quayecosystem <quayecosystemname>
----
+
[source,yaml]
----
metadata:
  labels:
    quay-operator/migrate: "true"
----
. Wait for a `QuayRegistry` to be created with the same `metadata.name` as your `QuayEcosystem`. The `QuayEcosystem` will be marked with the label `"quay-operator/migration-complete": "true"`.

. After the `status.registryEndpoint` of the new `QuayRegistry` is set, access {productname} and confirm that all data and settings were migrated successfully.

. If everything works correctly, you can delete the `QuayEcosystem` and Kubernetes garbage collection will clean up all old resources.

[id="reverting-quayecosystem-upgrade"]
=== Reverting QuayEcosystem Upgrade

If something goes wrong during the automatic upgrade from `QuayEcosystem` to `QuayRegistry`, follow these steps to revert back to using the `QuayEcosystem`:

.Procedure

. Delete the `QuayRegistry` using either the UI or `kubectl`:
+
[source,terminal]
----
$ kubectl delete -n <namespace> quayregistry <quayecosystem-name>
----

. If external access was provided using a `Route`, change the `Route` to point back to the original `Service` using the UI or `kubectl`.

[NOTE]
====
If your `QuayEcosystem` was managing the PostgreSQL database, the upgrade process will migrate your data to a new PostgreSQL database managed by the upgraded Operator. Your old database will not be changed or removed but {productname} will no longer use it once the migration is complete. If there are issues during the data migration, the upgrade process will exit and it is recommended that you continue with your database as an unmanaged component.
====

[id="supported-quayecossytem-configurations-for-upgrades"]
=== Supported QuayEcosystem Configurations for Upgrades

The {productname} Operator reports errors in its logs and in `status.conditions` if migrating a `QuayEcosystem` component fails or is unsupported. All unmanaged components should migrate successfully because no Kubernetes resources need to be adopted and all the necessary values are already provided in {productname}'s `config.yaml` file.

*Database*

Ephemeral database not supported (`volumeSize` field must be set).

*Redis*

Nothing special needed.

*External Access*

Only passthrough `Route` access is supported for automatic migration. Manual migration required for other methods.

* `LoadBalancer` without custom hostname:
After the `QuayEcosystem` is marked with label `"quay-operator/migration-complete": "true"`, delete the `metadata.ownerReferences` field from existing `Service` _before_ deleting the `QuayEcosystem` to prevent Kubernetes from garbage collecting the `Service` and removing the load balancer. A new `Service` will be created with `metadata.name` format `<QuayEcosystem-name>-quay-app`. Edit the `spec.selector` of the existing `Service` to match the `spec.selector` of the new `Service` so traffic to the old load balancer endpoint will now be directed to the new pods. You are now responsible for the old `Service`; the Quay Operator will not manage it.

* `LoadBalancer`/`NodePort`/`Ingress` with custom hostname:
A new `Service` of type `LoadBalancer` will be created with `metadata.name` format `<QuayEcosystem-name>-quay-app`. Change your DNS settings to point to the `status.loadBalancer` endpoint provided by the new `Service`.

*Clair*

Nothing special needed.

*Object Storage*

`QuayEcosystem` did not have a managed object storage component, so object storage will always be marked as unmanaged. Local storage is not supported.

*Repository Mirroring*

Nothing special needed.

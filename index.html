<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" class="chrometwo"><head><title>Red Hat Quay Architecture</title><link rel="stylesheet" type="text/css" href="Common_Content/css/default.css"/><meta name="generator" content="publican v4.3.2"/><meta name="description" content="Red Hat Quay Architecture"/><link rel="next" href="#arch-intro" title="Chapter 1. Red Hat Quay features"/><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><script type="text/javascript" src="Common_Content/scripts/jquery-1.7.1.min.js"> </script><script type="text/javascript" src="Common_Content/scripts/utils.js"> </script><script type="text/javascript" src="Common_Content/scripts/highlight.js/highlight.pack.js"> </script></head><body><div id="chrometwo"><div id="main"><div xml:lang="en-US" class="book" id="idm46223369423536"><div class="titlepage"><div><div class="producttitle"><span class="productname">Red Hat Quay</span> <span class="productnumber">3.5</span></div><div><h1 class="title">Red Hat Quay Architecture</h1></div><div><h2 class="subtitle">Red Hat Quay Architecture</h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat OpenShift Documentation Team</span></div></div><div><a href="#idm46223348122720">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				Red Hat Quay Architecture
			</div></div></div></div><hr/></div><div class="toc"><ul class="toc"><li><span class="chapter"><a href="#arch-intro">1. Red Hat Quay features</a></span></li><li><span class="chapter"><a href="#arch-core-intro">2. Core functionality</a></span><ul><li><span class="section"><a href="#infrastructure">2.1. Infrastructure</a></span><ul><li><span class="section"><a href="#quay_on_standalone_hosts">2.1.1. Quay on standalone hosts</a></span></li><li><span class="section"><a href="#quay_on_openshift">2.1.2. Quay on OpenShift</a></span></li><li><span class="section"><a href="#quay_on_public_cloud">2.1.3. Quay on public cloud</a></span></li><li><span class="section"><a href="#image_storage_backend">2.1.4. Image storage backend</a></span></li><li><span class="section"><a href="#database_backend">2.1.5. Database backend</a></span></li></ul></li><li><span class="section"><a href="#running_red_hat_quay_on_public_cloud">2.2. Running Red Hat Quay on Public Cloud</a></span><ul><li><span class="section"><a href="#running_red_hat_quay_on_aws">2.2.1. Running Red Hat Quay on AWS</a></span></li><li><span class="section"><a href="#running_red_hat_quay_on_microsoft_azure">2.2.2. Running Red Hat Quay on Microsoft Azure</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#security-intro">3. Security Overview</a></span><ul><li><span class="section"><a href="#clair_v4">3.1. Clair v4</a></span><ul><li><span class="section"><a href="#clair_v2_and_clair_v4_comparison">3.1.1. Clair v2 and Clair v4 Comparison</a></span></li><li><span class="section"><a href="#migrating_from_clair_v2_to_clair_v4">3.1.2. Migrating from Clair v2 to Clair v4</a></span></li></ul></li><li><span class="section"><a href="#vulnerability_scanning_via_clair">3.2. Vulnerability scanning via Clair</a></span></li></ul></li><li><span class="chapter"><a href="#content-distrib-intro">4. Content distribution</a></span><ul><li><span class="section"><a href="#mirroring-intro">4.1. Repository mirroring</a></span><ul><li><span class="section"><a href="#mirroring-using">4.1.1. Using repository mirroring</a></span></li><li><span class="section"><a href="#mirroring-recommend">4.1.2. Repository mirroring recommendations</a></span></li><li><span class="section"><a href="#mirroring-events">4.1.3. Event notifications for mirroring</a></span></li><li><span class="section"><a href="#mirroring-api-intro">4.1.4. Mirroring API</a></span></li></ul></li><li><span class="section"><a href="#georepl-intro">4.2. Geo-replication</a></span><ul><li><span class="section"><a href="#geo_replication_features">4.2.1. Geo-replication features</a></span></li><li><span class="section"><a href="#geo_replication_requirements_and_constraints">4.2.2. Geo-replication requirements and constraints</a></span></li><li><span class="section"><a href="#georepl-arch">4.2.3. Geo-replication architecture</a></span></li><li><span class="section"><a href="#georepl-mixed-storage">4.2.4. Mixed storage for geo-replication</a></span></li></ul></li><li><span class="section"><a href="#mirroring-versus-georepl">4.3. Repository mirroring versus geo-replication</a></span></li><li><span class="section"><a href="#airgap-intro">4.4. Air-gapped / disconnected deployments</a></span><ul><li><span class="section"><a href="#airgap-clair">4.4.1. Using Clair in air-gapped environments</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#access-control-intro">5. Access control</a></span></li><li><span class="chapter"><a href="#scalability-intro">6. Scalability</a></span></li><li><span class="chapter"><a href="#build-automation-intro">7. Build automation</a></span></li><li><span class="chapter"><a href="#integration-intro">8. Integration</a></span></li></ul></div><section class="chapter" id="arch-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Red Hat Quay features</h1></div></div></div><p>
			Red Hat Quay is a trusted, open source container registry platform that runs everywhere, but runs best on Red Hat OpenShift. It scales without limits, from a developer laptop to a container host or Kubernetes, and can be deployed on-premise or on public cloud. It provides global governance and security controls, with features including image vulnerability scanning, access controls, geo-replication and repository mirroring.
		</p><p>
			<span class="inlinemediaobject"><img src="images/quay-features.png" alt="Quay features"/></span>
		</p><p>
			This guide provides an insight into architectural patterns to use when deploying Red Hat Quay. It contains sizing guidance and deployment prerequisites, along with best practices for ensuring high availability for your Quay registry.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<a class="link" href="#arch-core-intro" title="Chapter 2. Core functionality">Core functionality</a>
				</li><li class="listitem">
					<a class="link" href="#security-intro" title="Chapter 3. Security Overview">Security</a>
				</li><li class="listitem">
					<a class="link" href="#content-distrib-intro" title="Chapter 4. Content distribution">Content distribution</a>
				</li><li class="listitem">
					<a class="link" href="#access-control-intro" title="Chapter 5. Access control">Access control</a>
				</li><li class="listitem">
					<a class="link" href="#build-automation-intro" title="Chapter 7. Build automation">Build automation</a>
				</li><li class="listitem">
					<a class="link" href="#scalability-intro" title="Chapter 6. Scalability">Scalability</a>
				</li><li class="listitem">
					<a class="link" href="#integration-intro" title="Chapter 8. Integration">Integration</a>
				</li></ul></div></section><section class="chapter" id="arch-core-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Core functionality</h1></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					High availability
				</li><li class="listitem">
					Full standards / spec support
				</li><li class="listitem">
					Long-Term protocol support
				</li><li class="listitem">
					OCI compatibility
				</li><li class="listitem">
					Enterprise grade support
				</li><li class="listitem">
					Regular updates
				</li></ul></div><section class="section" id="infrastructure"><div class="titlepage"><div><div><h2 class="title">2.1. Infrastructure</h2></div></div></div><p>
				Quay runs on any physical or virtual infrastructure, both on-premise or public cloud. Deployments range from simple to massively scaled, including:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						All-in-one setup on a developer laptop
					</li><li class="listitem">
						Highly available on OpenShift
					</li><li class="listitem">
						Geographically dispersed setup across multiple availability zones and regions
					</li></ul></div><section class="section" id="quay_on_standalone_hosts"><div class="titlepage"><div><div><h3 class="title">2.1.1. Quay on standalone hosts</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Poof-of-concept deployment, where Quay runs on the same machine as the image storage, database, Redis and optionally, Clair security scanning
						</li><li class="listitem">
							Highly available setups running on multiple hosts, using <code class="literal">systemd</code> to ensure restart on failure/reboot
						</li></ul></div><p>
					Standalone deployment is typically a manual process, but it can be automated using Ansible. All standalone hosts require valid RHEL subscriptions.
				</p></section><section class="section" id="quay_on_openshift"><div class="titlepage"><div><div><h3 class="title">2.1.2. Quay on OpenShift</h3></div></div></div><p>
					Running Quay on OpenShift provides many benefits:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Automated scaling and updates
						</li><li class="listitem">
							Quay Operator can manage Quay and all dependencies
						</li><li class="listitem">
							Automated deployment and Day 2 management of Red Hat Quay with customization options
						</li><li class="listitem">
							Integration with existing OpenShift processes like GitOps, monitoring, alerting, logging
						</li></ul></div><p>
					Quay can run on OpenShift infra nodes, meaning no further subscriptions are required.
				</p></section><section class="section" id="quay_on_public_cloud"><div class="titlepage"><div><div><h3 class="title">2.1.3. Quay on public cloud</h3></div></div></div><p>
					Quay can run on public clouds, either in standalone mode or where OpenShift itself has been deployed on public cloud.
				</p><p>
					Recommendation: If Quay is running on public cloud, then you should use the public cloud services for Quay backend services to ensure proper HA and scalability
				</p></section><section class="section" id="image_storage_backend"><div class="titlepage"><div><div><h3 class="title">2.1.4. Image storage backend</h3></div></div></div><p>
					Quay stores all binary blobs in its storage backend
				</p><p>
					Local storage and NFS only for PoC / test setups
				</p><p>
					Quay HA requires an HA storage setup
				</p><p>
					Geo-replication requires object storage and does not work with local storage
				</p><section class="section" id="supported_on_prem_storage_types"><div class="titlepage"><div><div><h4 class="title">2.1.4.1. Supported on-prem storage types</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Ceph Rados RGW
							</li><li class="listitem">
								OpenStack Swift
							</li><li class="listitem">
								RHODF 4 (via NooBaa)
							</li><li class="listitem">
								RHOCS 3 (via NooBaa) (TP) TODO Check
							</li></ul></div></section><section class="section" id="supported_public_cloud_storage_types"><div class="titlepage"><div><div><h4 class="title">2.1.4.2. Supported public cloud storage types</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								AWS S3
							</li><li class="listitem">
								Google Cloud Storage
							</li><li class="listitem">
								Azure Blob Storage
							</li></ul></div></section></section><section class="section" id="database_backend"><div class="titlepage"><div><div><h3 class="title">2.1.5. Database backend</h3></div></div></div><p>
					Quay stores most of its configuration and all metadata and logs inside its database backend. Logs can be pushed into ElasticSearch instead
				</p><p>
					PostgreSQL is the preferred database backend since it can be used for both Quay and Clair
				</p><p>
					Quay works fine with MySQL too (5.7+) but Clair requires PostgreSQL
				</p><p>
					Quay HA requires an HA database setup
				</p><p>
					If Quay is running on public cloud infrastructure, we recommend the use of the PostgreSQL services provided by your cloud provider.
				</p><p>
					Geo-replication requires a single, shared database that is accessible from all regions
				</p></section></section><section class="section" id="running_red_hat_quay_on_public_cloud"><div class="titlepage"><div><div><h2 class="title">2.2. Running Red Hat Quay on Public Cloud</h2></div></div></div><p>
				A full list of tested and supported configurations can be found in the Red Hat Quay Tested Integrations Matrix at <a class="link" href="https://access.redhat.com/articles/4067991">https://access.redhat.com/articles/4067991</a>
			</p><section class="section" id="running_red_hat_quay_on_aws"><div class="titlepage"><div><div><h3 class="title">2.2.1. Running Red Hat Quay on AWS</h3></div></div></div><p>
					<span class="inlinemediaobject"><img src="images/public-cloud-aws.png" alt="Red Hat Quay on AWS"/></span>
				</p><p>
					If Red Hat Quay is running on AWS, you can use
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							AWS Elastic Load Balancer
						</li><li class="listitem">
							AWS S3 (hot) blob storage
						</li><li class="listitem">
							AWS RDS database
						</li><li class="listitem">
							AWS ElastiCache Redis
						</li><li class="listitem">
							EC2 VMs recommendation: M3.Large or M4.XLarge
						</li></ul></div></section><section class="section" id="running_red_hat_quay_on_microsoft_azure"><div class="titlepage"><div><div><h3 class="title">2.2.2. Running Red Hat Quay on Microsoft Azure</h3></div></div></div><p>
					<span class="inlinemediaobject"><img src="images/public-cloud-azure.png" alt="Red Hat Quay on Microsoft Azure"/></span>
				</p><p>
					If Quay is running on Microsoft Azure, you can use:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Azure managed services such as HA PostgreSQL
						</li><li class="listitem">
							Azure Blob Storage must be hot storage (not Azure Cool Blob Storage)
						</li><li class="listitem">
							Azure Cache for Redis
						</li></ul></div></section></section></section><section class="chapter" id="security-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Security Overview</h1></div></div></div><p>
			Red Hat Quay is built for real enterprise use cases where content governance and security are two major focus areas. {produtname} content governance and security includes a built-in vulnerability scanning via Clair.
		</p><p>
			Clair is an open source tool developed by CoreOS for Quay that generates analyses of vulnerabilities in application containers, which currently includes Open Container Initiative (OCI) and Docker images. Clients that use the Clair API to index their container images can then match their images against known vulnerabilities.
		</p><p>
			Clair supports the extraction of contents and assignment of vulnerabilities from the following official base containers:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Ubuntu Linux
				</li><li class="listitem">
					Debian Linux
				</li><li class="listitem">
					Red Hat Enterprise Linux
				</li><li class="listitem">
					SUSE
				</li><li class="listitem">
					Oracle Linux
				</li><li class="listitem">
					Alpine Linux
				</li><li class="listitem">
					Amazon Linux
				</li><li class="listitem">
					VMWare Photon
				</li><li class="listitem">
					Python
				</li></ul></div><p>
			Clair’s analysis can be broken down into three distinct parts:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
					<span class="strong strong"><strong>Indexing</strong></span>: Indexing starts with submitting a <code class="literal">Manifest</code> to Clair. On receipt, Clair will fetch layers, scan their contents, and return an intermediate representation called an <code class="literal">IndexReport</code>.
				</p><p class="simpara">
					Manifests are Clair’s representation of a container image. Clair leverages the fact <code class="literal">OCI Manifests</code> and <code class="literal">Layers</code> are content-addressed to reduce duplicated work.
				</p><p class="simpara">
					Once a <code class="literal">Manifest</code> is indexed, the <code class="literal">IndexReport</code> is persisted for later retrieval.
				</p></li><li class="listitem"><p class="simpara">
					<span class="strong strong"><strong>Matching</strong></span>: Matching is taking an <code class="literal">IndexReport</code> and correlating vulnerabilities affecting the <code class="literal">Manifest</code> the report represents.
				</p><p class="simpara">
					Clair continuously ingests new security data and a request to the matcher will always provide users with the most to date vulnerability analysis of an <code class="literal">IndexReport</code>.
				</p></li><li class="listitem">
					<span class="strong strong"><strong>Notifications</strong></span>: Clair implements a notification service. When new vulnerabilities are discovered, the notifier service will determine if these vulnerabilities affect any indexed <code class="literal">Manifests</code>. The notifier will then take action according to its configuration.
				</li></ul></div><section class="section" id="clair_v4"><div class="titlepage"><div><div><h2 class="title">3.1. Clair v4</h2></div></div></div><p>
				Released with Red Hat Quay 3.4, Clair v4 is the latest version of Clair. It is built on a new architecture consisting of Clair Core and a service wrapper. Clair v4 made several enhancements to Clair v2, including:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Support for the Python programming language package. Support for additional languages is planned for future versions of Clair and Red Hat Quay.
					</li><li class="listitem">
						Immutable data model and a new manifest-oriented API.
					</li><li class="listitem">
						Refocus on the latest Open Container Initiative (OCI) specifications.
					</li><li class="listitem">
						Image hashes and layer hashes are now treated as content addressable, so that images are uniquely identified as a whole.
					</li></ul></div><section class="section" id="clair_v2_and_clair_v4_comparison"><div class="titlepage"><div><div><h3 class="title">3.1.1. Clair v2 and Clair v4 Comparison</h3></div></div></div><div class="table" id="idm46223371059968"><p class="title"><strong>Table 3.1. Kubernetes services that run on the control plane</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"/><col style="width: 67%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm46223372237232" scope="col">Component</th><th align="left" valign="top" id="idm46223372282112" scope="col">Clair v2</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm46223372237232">
								<p>
									Clair v4
								</p>
								</td><td align="left" valign="top" headers="idm46223372282112">
								<p>
									API layers
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm46223372237232">
								<p>
									In Clair v2, clients were required to provide layers to the API.
								</p>
								</td><td align="left" valign="top" headers="idm46223372282112">
								<p>
									In Clair v4 is manifest-based, providing an easier API for users.
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm46223372237232">
								<p>
									Insights and reports
								</p>
								</td><td align="left" valign="top" headers="idm46223372282112">
								<p>
									Clair v2 provided only insights on vulnerabilities
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm46223372237232">
								<p>
									Clair v4 provides detailed reports on the content of the container, which can be fed to other tools for analyses or inventory purposes.
								</p>
								</td><td align="left" valign="top" headers="idm46223372282112">
								<p>
									Architecture
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm46223372237232">
								<p>
									Clair v2 ran as a monolithic application.
								</p>
								</td><td align="left" valign="top" headers="idm46223372282112">
								<p>
									Clair v4 divides functionality across multiple services for ease of development and scaling use cases.
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm46223372237232">
								<p>
									Support for language packages
								</p>
								</td><td align="left" valign="top" headers="idm46223372282112">
								<p>
									Clair v2 does not support computer language packages.
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm46223372237232">
								<p>
									Clair v4 supports Python language packages, with plans of adding more in future versions.
								</p>
								</td><td align="left" valign="top" headers="idm46223372282112">
								<p>
									Package locator
								</p>
								</td></tr><tr><td align="left" valign="top" headers="idm46223372237232">
								<p>
									Clair v2 did not provide details on where packages were located inside of the container.
								</p>
								</td><td align="left" valign="top" headers="idm46223372282112">
								<p>
									Clair v4 identifies where packages are located inside of the container.
								</p>
								</td></tr></tbody></table></div></div></section><section class="section" id="migrating_from_clair_v2_to_clair_v4"><div class="titlepage"><div><div><h3 class="title">3.1.2. Migrating from Clair v2 to Clair v4</h3></div></div></div><p>
					Starting with Red Hat Quay 3.4, Clair v4 will be used by default. It will also be the only version of Clair continuously supported, as older Red Hat Quay versions are not supported with Clair v4 in production. Users should continue using Clair v2 if using a version of Red Hat Quay earlier than 3.4.
				</p><p>
					Existing Red Hat Quay 3.3 deployments will be upgraded to Clair v4 when managed via the Red Hat Quay Operator. Manually upgraded Red Hat Quay deployments can install Clair v4 side-by-side, which will cause the following:
				</p><p>
					+ * All new image vulnerability scans to be performed by Clair v4 * Existing images to be rescanned by Clair v4
				</p><p>
					logging and auditing, and notifications and alerting, and other features to federate content across your clusters, data centers, and regions.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Vulnerability scanning
						</li><li class="listitem">
							Logging and auditing
						</li><li class="listitem">
							Notifications and alerting
						</li></ul></div></section></section><section class="section" id="vulnerability_scanning_via_clair"><div class="titlepage"><div><div><h2 class="title">3.2. Vulnerability scanning via Clair</h2></div></div></div></section></section><section class="chapter" id="content-distrib-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Content distribution</h1></div></div></div><p>
			Content distribution features in Red Hat Quay include:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<a class="link" href="#mirroring-intro" title="4.1. Repository mirroring">Repository mirroring</a>
				</li><li class="listitem">
					<a class="link" href="#georepl-intro" title="4.2. Geo-replication">Geo-replication</a>
				</li><li class="listitem">
					<a class="link" href="#airgap-intro" title="4.4. Air-gapped / disconnected deployments">Deployment in air-gapped environments</a>
				</li></ul></div><section class="section" id="mirroring-intro"><div class="titlepage"><div><div><h2 class="title">4.1. Repository mirroring</h2></div></div></div><p>
				Red Hat Quay repository mirroring lets you mirror images from external container registries (or another local registry) into your Red Hat Quay cluster. Using repository mirroring, you can synchronize images to Red Hat Quay based on repository names and tags.
			</p><p>
				From your Red Hat Quay cluster with repository mirroring enabled, you can:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Choose a repository from an external registry to mirror
					</li><li class="listitem">
						Add credentials to access the external registry
					</li><li class="listitem">
						Identify specific container image repository names and tags to sync
					</li><li class="listitem">
						Set intervals at which a repository is synced
					</li><li class="listitem">
						Check the current state of synchronization
					</li></ul></div><p>
				To use the mirroring functionality, you need to:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Enable Repository Mirroring in the Red Hat Quay configuration
					</li><li class="listitem">
						Run a repository mirroring worker
					</li><li class="listitem">
						Create mirrored repositories
					</li></ul></div><p>
				All repository mirroring configuration can be performed using the configuration tool UI or via the Quay API
			</p><section class="section" id="mirroring-using"><div class="titlepage"><div><div><h3 class="title">4.1.1. Using repository mirroring</h3></div></div></div><p>
					Here are some features and limitations of Red Hat Quay repository mirroring:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							With repository mirroring, you can mirror an entire repository or selectively limit which images are synced. Filters can be based on a comma-separated list of tags, a range of tags, or other means of identifying tags through regular expressions.
						</li><li class="listitem">
							Once a repository is set as mirrored, you cannot manually add other images to that repository.
						</li><li class="listitem">
							Because the mirrored repository is based on the repository and tags you set, it will hold only the content represented by the repo/tag pair. In other words, if you change the tag so that some images in the repository no longer match, those images will be deleted.
						</li><li class="listitem">
							Only the designated robot can push images to a mirrored repository, superseding any role-based access control permissions set on the repository.
						</li><li class="listitem">
							With a mirrored repository, a user can pull images (given read permission) from the repository but not push images to the repository.
						</li><li class="listitem">
							Changing settings on your mirrored repository is done from the Mirrors tab on the Repositories page for the mirrored repository you create.
						</li><li class="listitem">
							Images are synced at set intervals, but can also be synced on demand.
						</li></ul></div></section><section class="section" id="mirroring-recommend"><div class="titlepage"><div><div><h3 class="title">4.1.2. Repository mirroring recommendations</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Repository mirroring pods can run on any node including other nodes where Quay is already running
						</li><li class="listitem">
							Repository mirroring is scheduled in the database and run in batches. As a result, more workers could mean faster mirroring, since more batches will be processed.
						</li><li class="listitem"><p class="simpara">
							The optimal number of mirroring pods depends on:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									The total number of repositories to be mirrored
								</li><li class="listitem">
									The number of images and tags in the repositories and the frequency of changes
								</li><li class="listitem">
									Parallel batches
								</li></ul></div></li><li class="listitem">
							You should balance your mirroring schedule across all mirrored repositories, so that they do not all start up at the same time.
						</li><li class="listitem">
							For a mid-size deployment, with approximately 1000 users and 1000 repositories, and with roughly 100 mirrored repositories, it is expected that you would use 3-5 mirroring pods, scaling up to 10 if required.
						</li></ul></div></section><section class="section" id="mirroring-events"><div class="titlepage"><div><div><h3 class="title">4.1.3. Event notifications for mirroring</h3></div></div></div><p>
					There are three notification events for repository mirroring:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Repository Mirror Started
						</li><li class="listitem">
							Repository Mirror Success
						</li><li class="listitem">
							Repository Mirror Unsuccessful
						</li></ul></div><p>
					The events can be configured inside the Settings tab for each repository, and all existing notification methods such as email, slack, Quay UI and webhooks are supported.
				</p></section><section class="section" id="mirroring-api-intro"><div class="titlepage"><div><div><h3 class="title">4.1.4. Mirroring API</h3></div></div></div><p>
					You can use the Quay API to configure repository mirroring:
				</p><p>
					<span class="inlinemediaobject"><img src="images/swagger-mirroring.png" alt="Mirroring API"/></span>
				</p><p>
					More information is available in the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html-single/red_hat_quay_api_guide/index">Red Hat Quay API Guide</a>
				</p></section></section><section class="section" id="georepl-intro"><div class="titlepage"><div><div><h2 class="title">4.2. Geo-replication</h2></div></div></div><p>
				Geo-replication allows multiple, geographically distributed Quay deployments to work as a single registry from the perspective of a client or user. It significantly improves push and pull performance in a globally-distributed Quay setup. Image data is asynchronously replicated in the background with transparent failover / redirect for clients.
			</p><section class="section" id="geo_replication_features"><div class="titlepage"><div><div><h3 class="title">4.2.1. Geo-replication features</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							When geo-replication is configured, container image pushes will be written to the preferred storage engine for that Red Hat Quay instance (typically the nearest storage backend within the region).
						</li><li class="listitem">
							After the initial push, image data will be replicated in the background to other storage engines.
						</li><li class="listitem">
							The list of replication locations is configurable and those can be different storage backends.
						</li><li class="listitem">
							An image pull will always use the closest available storage engine, to maximize pull performance.
						</li><li class="listitem">
							If replication hasn’t been completed yet, the pull will use the source storage backend instead.
						</li></ul></div></section><section class="section" id="geo_replication_requirements_and_constraints"><div class="titlepage"><div><div><h3 class="title">4.2.2. Geo-replication requirements and constraints</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A single database, and therefore all metadata and Quay configuration, is shared across all regions.
						</li><li class="listitem">
							A single Redis cache is shared across the entire Quay setup and needs to accessible by all Quay pods.
						</li><li class="listitem">
							The exact same configuration should be used across all regions, with exception of the storage backend, which can be configured explicitly using the <code class="literal">QUAY_DISTRIBUTED_STORAGE_PREFERENCE</code> environment variable.
						</li><li class="listitem">
							Geo-Replication requires object storage in each region. It does not work with local storage or NFS.
						</li><li class="listitem">
							Each region must be able to access every storage engine in each region (requires a network path).
						</li><li class="listitem">
							Alternatively, the storage proxy option can be used.
						</li><li class="listitem">
							The entire storage backend (all blobs) is replicated. This is in contrast to repository mirroring, which can be limited to an organization or repository or image.
						</li><li class="listitem">
							All Quay instances must share the same entrypoint, typically via load balancer.
						</li><li class="listitem">
							All Quay instances must have the same set of superusers, as they are defined inside the common configuration file.
						</li></ul></div><p>
					If the above requirements cannot be met, you should instead use two or more distinct Quay deployments and take advantage of repository mirroring functionality.
				</p></section><section class="section" id="georepl-arch"><div class="titlepage"><div><div><h3 class="title">4.2.3. Geo-replication architecture</h3></div></div></div><p>
					<span class="inlinemediaobject"><img src="images/georeplication-aws.png" alt="Georeplication"/></span>
				</p><p>
					In the example shown above, Quay is running in two separate regions, with a common database and a common Redis instance. Localized image storage is provided in each region and image pulls are served from the closest available storage engine. Container image pushes are written to the preferred storage engine for the Quay instance, and will then be replicated, in the background, to the other storage engines.
				</p><p>
					The following block diagram shows a possible distribution of resources where Quay has been deployed on two OpenShift clusters using the Operator:
				</p><p>
					<span class="inlinemediaobject"><img src="images/georeplication-arch.png" alt="Georeplication architecture"/></span>
				</p></section><section class="section" id="georepl-mixed-storage"><div class="titlepage"><div><div><h3 class="title">4.2.4. Mixed storage for geo-replication</h3></div></div></div><p>
					Quay geo-replication supports the use of different, and multiple, replication targets for example, using AWS S3 storage on public cloud and using Ceph storage on-prem. This complicates the key requirement of granting access to all storage backends from all Quay pods and cluster nodes. As a result, it is recommended that you:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Use a VPN to prevent visibility of the internal storage <span class="emphasis"><em>or</em></span>
						</li><li class="listitem">
							Use a token pair that only allows access to the specified bucket used by Quay
						</li></ul></div><p>
					This will result in the public cloud instance of Quay having access to on-prem storage but the network will be encrypted, protected, and will use ACLs, thereby meeting security requirements.
				</p><p>
					If you cannot implement these security measures, it may be preferable to deploy two distinct Quay registries and to use repository mirroring as an alternative to geo-replication.
				</p></section></section><section class="section" id="mirroring-versus-georepl"><div class="titlepage"><div><div><h2 class="title">4.3. Repository mirroring versus geo-replication</h2></div></div></div><p>
				Quay geo-replication mirrors the entire image storage backend data between 2 or more different storage backends while the database is shared (one Quay registry with two different blob storage endpoints). The primary use cases for geo-replication are:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Speeding up access to the binary blobs for geographically dispersed setups
					</li><li class="listitem">
						Guaranteeing that the image content is the same across regions
					</li></ul></div><p>
				Repository mirroring synchronizes selected repositories (or subsets of repositories) from one registry to another. The registries are distinct, with registry is separate database and image storage. The primary use cases for mirroring are:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Independent registry deployments in different datacenters or regions, where a certain subset of the overall content is supposed to be shared across the datacenters / regions
					</li><li class="listitem">
						Automatic synchronization or mirroring of selected (whitelisted) upstream repositories from external registries into a local Quay deployment
					</li></ul></div><div class="informalexample"><p>
				Repository mirroring and geo-replication can be used simultaneously.
			</p></div><div class="table" id="idm46223435439408"><p class="title"><strong>Table 4.1. Red Hat Quay Repository mirroring versus geo-replication</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"/><col style="width: 33%; " class="col_2"/><col style="width: 33%; " class="col_3"/></colgroup><thead><tr><th align="left" valign="top" id="idm46223346960368" scope="col">Feature / Capability</th><th align="left" valign="top" id="idm46223346959504" scope="col">Geo-replication</th><th align="left" valign="top" id="idm46223346434928" scope="col">Repository mirroring</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm46223346960368">
							<p>
								What is the feature designed to do?
							</p>
							</td><td align="left" valign="top" headers="idm46223346959504">
							<p>
								A shared, global registry
							</p>
							</td><td align="left" valign="top" headers="idm46223346434928">
							<p>
								Distinct, different registries
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46223346960368">
							<p>
								What happens if replication or mirroring hasn’t been completed yet?
							</p>
							</td><td align="left" valign="top" headers="idm46223346959504">
							<p>
								The remote copy is used (slower)
							</p>
							</td><td align="left" valign="top" headers="idm46223346434928">
							<p>
								No image is served
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46223346960368">
							<p>
								Is access to all storage backends in both regions required?
							</p>
							</td><td align="left" valign="top" headers="idm46223346959504">
							<p>
								Yes (all Red Hat Quay nodes)
							</p>
							</td><td align="left" valign="top" headers="idm46223346434928">
							<p>
								No (distinct storage)
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46223346960368">
							<p>
								Can users push images from both sites to the same repository?
							</p>
							</td><td align="left" valign="top" headers="idm46223346959504">
							<p>
								Yes
							</p>
							</td><td align="left" valign="top" headers="idm46223346434928">
							<p>
								No
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46223346960368">
							<p>
								Is all registry content and configuration identical across all regions (shared database)
							</p>
							</td><td align="left" valign="top" headers="idm46223346959504">
							<p>
								Yes
							</p>
							</td><td align="left" valign="top" headers="idm46223346434928">
							<p>
								No
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46223346960368">
							<p>
								Can users select individual namespaces or repositories to be mirrored?
							</p>
							</td><td align="left" valign="top" headers="idm46223346959504">
							<p>
								No,by default
							</p>
							</td><td align="left" valign="top" headers="idm46223346434928">
							<p>
								Yes
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46223346960368">
							<p>
								Can users apply filters to synchronization rules?
							</p>
							</td><td align="left" valign="top" headers="idm46223346959504">
							<p>
								No
							</p>
							</td><td align="left" valign="top" headers="idm46223346434928">
							<p>
								Yes
							</p>
							</td></tr></tbody></table></div></div></section><section class="section" id="airgap-intro"><div class="titlepage"><div><div><h2 class="title">4.4. Air-gapped / disconnected deployments</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Quay and Clair connected to the internet, with an air-gapped OpenShift cluster accessing the Quay registry through an explicit, white-listed hole in the firewall
					</li><li class="listitem">
						Quay and Clair running inside the firewall, with image and CVE data transferred to the target system using offline media. The data is exported from a separate Quay and Clair deployment that is connected to the internet.
					</li></ul></div><p>
				<span class="inlinemediaobject"><img src="images/air-gap.png" alt="Air-gapped deployment"/></span>
			</p><section class="section" id="airgap-clair"><div class="titlepage"><div><div><h3 class="title">4.4.1. Using Clair in air-gapped environments</h3></div></div></div><p>
					By default, Clair will attempt to run automated updates against Red Hat servers. To run Clair in network environments that are disconnected from the internet:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Disable Clair auto-update in the Clair configuration bundle
						</li><li class="listitem">
							Manually update the vulnerability database on a system with internet access and then export to disk
						</li><li class="listitem">
							Transfer the on-disk data to the target system using offline media and then manually import it into Clair
						</li></ul></div><p>
					Using Clair in air-gapped environments is fully containerized and, as a result, is easy to automate.
				</p></section></section></section><section class="chapter" id="access-control-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Access control</h1></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Authentication providers
				</li><li class="listitem">
					Fine-grained RBAC
				</li><li class="listitem">
					Organizations and teams
				</li></ul></div></section><section class="chapter" id="scalability-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Scalability</h1></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Massive scale testing Quay.io
				</li><li class="listitem">
					Real-time garbage collection
				</li><li class="listitem">
					Automated squashing
				</li></ul></div></section><section class="chapter" id="build-automation-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Build automation</h1></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Seamless Git integration
				</li><li class="listitem">
					Build workers
				</li><li class="listitem">
					Webhooks
				</li></ul></div></section><section class="chapter" id="integration-intro"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Integration</h1></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Extensible API
				</li><li class="listitem">
					Webhooks, OAuth
				</li><li class="listitem">
					Robot Accounts
				</li></ul></div></section><div><div xml:lang="en-US" class="legalnotice" id="idm46223348122720"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"/>© 2021 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div></div></div><script type="text/javascript">
                        jQuery(document).ready(function() {
                            initSwitchery();
                            jQuery('pre[class*="language-"]').each(function(i, block){hljs.highlightBlock(block);});
                        });
                    </script></body></html>